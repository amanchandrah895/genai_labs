{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ohdrUjz-ugs6",
        "outputId": "94085d3e-5610-43f7-cf69-8a52e7458d85"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: groq in /usr/local/lib/python3.12/dist-packages (1.0.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from groq) (4.12.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from groq) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from groq) (0.28.1)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from groq) (2.12.3)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from groq) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.12/dist-packages (from groq) (4.15.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->groq) (3.11)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->groq) (2026.1.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->groq) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->groq) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->groq) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->groq) (0.4.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install groq\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from groq import Groq\n",
        "\n",
        "client = Groq(api_key=\"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\")  # paste your key here\n",
        "MODEL = \"openai/gpt-oss-safeguard-20b\"\n",
        "\n"
      ],
      "metadata": {
        "id": "wl5Eznjqum-T"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 1: Basic prompt\n",
        "\n",
        "prompt_v1 = \"Explain Machine Learning.\"\n",
        "\n",
        "response_v1 = client.chat.completions.create(\n",
        "    model=MODEL,\n",
        "    messages=[{\"role\": \"user\", \"content\": prompt_v1}]\n",
        ")\n",
        "\n",
        "print(\"STEP 1 OUTPUT:\\n\")\n",
        "print(response_v1.choices[0].message.content)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IpxSMFq9wcHQ",
        "outputId": "2a801fa2-c225-47f8-e8d1-7418bd0fa97d"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "STEP 1 OUTPUT:\n",
            "\n",
            "## What is Machine Learning?\n",
            "\n",
            "**Machine Learning (ML)** is a branch of artificial intelligence that teaches computers to improve their performance on a task by learning from data rather than being explicitly programmed for every possible scenario. Instead of writing rules that cover every edge case, we give a model a set of examples, let it discover patterns, and then let it use those patterns to make predictions or decisions on new, unseen data.\n",
            "\n",
            "---\n",
            "\n",
            "### Core Idea\n",
            "\n",
            "1. **Data** – The raw information you have (images, text, numbers, sensor readings, etc.).\n",
            "2. **Model** – A mathematical function or algorithm that can process the data and output something useful (a label, a number, an action).\n",
            "3. **Learning** – The process of adjusting the model’s internal parameters so that it performs better on a given objective (e.g., minimizing classification error).\n",
            "\n",
            "Think of it as training a student: you give the student many examples and a grading rubric, and over time the student gets better at answering new questions.\n",
            "\n",
            "---\n",
            "\n",
            "## The Basic ML Pipeline\n",
            "\n",
            "| Step | What Happens | Typical Tools / Concepts |\n",
            "|------|--------------|--------------------------|\n",
            "| **Collect** | Gather labeled or unlabeled data. | Databases, sensors, web scraping. |\n",
            "| **Pre‑process** | Clean, normalize, encode. | Pandas, scikit‑learn’s `StandardScaler`, tokenization. |\n",
            "| **Split** | Train / validation / test sets. | Train/test split, cross‑validation. |\n",
            "| **Choose a Model** | Select algorithm appropriate for the task. | Linear regression, decision trees, neural nets. |\n",
            "| **Train** | Optimize the model’s parameters on the training set. | Gradient descent, back‑propagation. |\n",
            "| **Validate** | Tune hyper‑parameters and avoid overfitting. | Grid search, Bayesian optimization. |\n",
            "| **Test** | Final performance estimate on unseen data. | Accuracy, ROC‑AUC, RMSE. |\n",
            "| **Deploy** | Serve the model in production. | REST API, edge deployment. |\n",
            "| **Monitor & Retrain** | Keep track of performance drift and update. | Online learning, drift detection. |\n",
            "\n",
            "---\n",
            "\n",
            "## Major Categories of ML\n",
            "\n",
            "| Category | How it works | Typical use‑cases |\n",
            "|----------|--------------|--------------------|\n",
            "| **Supervised learning** | Learns a mapping from inputs *x* to outputs *y* using labeled examples. | Spam detection, image classification, regression of house prices. |\n",
            "| **Unsupervised learning** | Finds structure or patterns without explicit labels. | Clustering customers, anomaly detection, dimensionality reduction (PCA, t‑SNE). |\n",
            "| **Semi‑supervised learning** | Combines a small set of labeled data with a large unlabeled set to improve performance. | Text classification when labeling is expensive. |\n",
            "| **Self‑supervised learning** | Uses the data itself to create proxy labels (e.g., predicting missing words). | Pre‑training large language models, contrastive learning. |\n",
            "| **Reinforcement learning** | Learns a policy by interacting with an environment to maximize cumulative reward. | Game playing, robotic control, recommendation systems. |\n",
            "| **Transfer learning** | Re‑uses a model trained on one task to speed up learning on another. | Fine‑tuning ImageNet models for medical imaging. |\n",
            "\n",
            "---\n",
            "\n",
            "## Key Concepts\n",
            "\n",
            "| Concept | Short Definition | Example |\n",
            "|---------|------------------|---------|\n",
            "| **Features** | Input variables that capture useful aspects of the data. | Pixel intensity, word embeddings. |\n",
            "| **Model Parameters** | Numbers learned during training that shape the model’s predictions. | Weights in a neural network. |\n",
            "| **Loss Function** | Quantifies how far predictions are from true values; we minimize it. | Cross‑entropy loss for classification. |\n",
            "| **Optimizer** | Algorithm that updates parameters to reduce loss. | SGD, Adam. |\n",
            "| **Regularization** | Techniques to prevent overfitting (e.g., L1/L2, dropout). | Adding weight decay. |\n",
            "| **Evaluation Metric** | Measure used to judge model quality. | Accuracy, precision, recall, F1, mean squared error. |\n",
            "\n",
            "---\n",
            "\n",
            "## Typical Algorithms\n",
            "\n",
            "| Task | Algorithm | Why it’s popular |\n",
            "|------|-----------|-------------------|\n",
            "| **Classification** | Logistic regression, Random Forests, SVM, Neural Nets (CNNs, transformers) | Handles a wide range of problems; deep nets excel with raw data. |\n",
            "| **Regression** | Linear regression, Gradient Boosting (XGBoost), Deep nets | Predict continuous outcomes. |\n",
            "| **Clustering** | K‑means, DBSCAN, Hierarchical | Find natural groupings. |\n",
            "| **Dimensionality Reduction** | PCA, t‑SNE, UMAP | Visualize high‑dimensional data. |\n",
            "| **Sequence Modelling** | RNN, LSTM, Transformer | Natural language, time series. |\n",
            "| **Reinforcement** | Q‑learning, Policy gradients, Actor‑Critic | Control tasks, games. |\n",
            "\n",
            "---\n",
            "\n",
            "## Common Challenges\n",
            "\n",
            "1. **Overfitting** – Model memorizes training data and fails on new data.\n",
            "2. **Bias & Fairness** – Models may inherit or amplify biases in training data.\n",
            "3. **Data quality** – Missing, noisy, or imbalanced data hurts performance.\n",
            "4. **Interpretability** – Complex models (deep nets) are hard to explain.\n",
            "5. **Scalability** – Training on massive datasets requires hardware and algorithmic tricks.\n",
            "\n",
            "---\n",
            "\n",
            "## A Simple Real‑World Example\n",
            "\n",
            "*Goal:* Predict whether a customer will churn (leave) from a subscription service.\n",
            "\n",
            "1. **Collect**: Past customer records (usage, support tickets, demographics).\n",
            "2. **Pre‑process**: Encode categorical fields, impute missing values.\n",
            "3. **Model**: Train a logistic regression or gradient‑boosted trees.\n",
            "4. **Validate**: Use 5‑fold cross‑validation; tune learning rate and depth.\n",
            "5. **Evaluate**: ROC‑AUC ≈ 0.85 on the test set.\n",
            "6. **Deploy**: Serve the model via a REST API to flag high‑risk customers.\n",
            "7. **Monitor**: Track churn predictions versus actual churn; retrain monthly.\n",
            "\n",
            "---\n",
            "\n",
            "## Bottom Line\n",
            "\n",
            "Machine learning is a data‑driven approach to building intelligent systems. By providing a model with enough relevant examples and a clear objective, we enable it to automatically discover the best way to solve a problem—often achieving performance that would be infeasible for a human to codify by hand. Whether it’s spotting spam emails, translating languages, or steering an autonomous car, ML turns raw data into actionable knowledge.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 2: Persona Prompt\n",
        "# YOU explicitly define who the AI should act as\n",
        "\n",
        "prompt_v2 = (\n",
        "    \"You are a senior Machine Learning professor teaching beginners.\\n\"\n",
        "    \"Explain Machine Learning in simple terms.\"\n",
        ")\n",
        "\n",
        "response_v2 = client.chat.completions.create(\n",
        "    model=MODEL,\n",
        "    messages=[{\"role\": \"user\", \"content\": prompt_v2}]\n",
        ")\n",
        "\n",
        "print(\"\\nSTEP 2 OUTPUT:\\n\")\n",
        "print(response_v2.choices[0].message.content)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EGkmM_R_xE22",
        "outputId": "b8785ba5-6a78-4ce9-ae0e-a6b99cf5e3b6"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "STEP 2 OUTPUT:\n",
            "\n",
            "## What is Machine Learning?  \n",
            "> *“Teaching computers to learn from experience, just like we do.”*  \n",
            "\n",
            "Think of a child who sees a picture of a cat and is told “That’s a cat.” The next time the child sees a new picture, they can guess whether it is a cat or not.  \n",
            "Machine Learning (ML) does the same thing for computers: it gives them **examples** and then lets them figure out the rules on their own.\n",
            "\n",
            "---\n",
            "\n",
            "## 1. The Ingredients\n",
            "\n",
            "| Ingredient | What it is | Why it matters |\n",
            "|------------|------------|----------------|\n",
            "| **Data** | A collection of real‑world examples (images, text, numbers). | The “experience” the model learns from. |\n",
            "| **Model** | A mathematical function that maps inputs to outputs (e.g., a neural network, decision tree). | The “student” that tries to discover patterns. |\n",
            "| **Algorithm** | A procedure that tweaks the model based on data (e.g., gradient descent). | The teacher that guides learning. |\n",
            "| **Loss / Error** | A numerical score that tells how wrong the model’s predictions are. | A feedback signal used to improve the model. |\n",
            "\n",
            "---\n",
            "\n",
            "## 2. The Learning Cycle (in plain English)\n",
            "\n",
            "1. **Show** the model a bunch of labelled examples.  \n",
            "   *E.g., show 1,000 pictures of cats and dogs, each tagged with “cat” or “dog.”*\n",
            "\n",
            "2. **Guess** what the labels should be for new data.  \n",
            "   *The model makes a prediction for every picture.*\n",
            "\n",
            "3. **Check** how good the guesses are by comparing them to the real labels.  \n",
            "   *If it guessed “dog” for a cat, that’s a mistake.*\n",
            "\n",
            "4. **Adjust** the model slightly to reduce mistakes.  \n",
            "   *This is done automatically by an algorithm.*\n",
            "\n",
            "5. **Repeat** until the model is “good enough” or the improvements become tiny.\n",
            "\n",
            "That’s the core of supervised learning. There are other flavors—unsupervised (discovering hidden structure without labels) and reinforcement (learning by trial‑and‑error in an environment)—but the cycle above is a great starting point.\n",
            "\n",
            "---\n",
            "\n",
            "## 3. Everyday Examples\n",
            "\n",
            "| Task | What the model learns | Example |\n",
            "|------|-----------------------|---------|\n",
            "| **Spam filter** | Identify words that usually appear in spam | Email labeled “spam” vs. “not spam” |\n",
            "| **Image recognition** | Recognize visual patterns | Dog vs. cat pictures |\n",
            "| **Speech recognition** | Map audio waves to words | “Hello” → audio signal |\n",
            "| **Recommendation** | Predict what you’ll like based on past likes | Netflix movie suggestions |\n",
            "\n",
            "---\n",
            "\n",
            "## 4. Key Takeaways\n",
            "\n",
            "- **ML is data‑driven**: The more quality data you give, the better the model can learn.\n",
            "- **It’s about pattern discovery**: ML models automatically find regularities that would be hard to hand‑code.\n",
            "- **Evaluation matters**: We test a model on new, unseen data to see if it truly learned or just memorized the training set.\n",
            "- **Not a magic wand**: Good ML requires careful choice of data, models, and validation. The results are probabilistic, not absolute.\n",
            "\n",
            "---\n",
            "\n",
            "## 5. A Quick “Hello, World” Code Snippet (Python + Scikit‑Learn)\n",
            "\n",
            "```python\n",
            "from sklearn.datasets import load_iris\n",
            "from sklearn.model_selection import train_test_split\n",
            "from sklearn.ensemble import RandomForestClassifier\n",
            "from sklearn.metrics import accuracy_score\n",
            "\n",
            "# 1. Load data\n",
            "X, y = load_iris(return_X_y=True)\n",
            "\n",
            "# 2. Split into train / test\n",
            "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
            "\n",
            "# 3. Pick a model\n",
            "clf = RandomForestClassifier(n_estimators=100)\n",
            "\n",
            "# 4. Train\n",
            "clf.fit(X_train, y_train)\n",
            "\n",
            "# 5. Predict\n",
            "y_pred = clf.predict(X_test)\n",
            "\n",
            "# 6. Evaluate\n",
            "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
            "```\n",
            "\n",
            "You just trained a classifier that can distinguish between three types of flowers using only four measurements—no physics or biology knowledge required!\n",
            "\n",
            "---\n",
            "\n",
            "## 6. What You’ll Learn Next\n",
            "\n",
            "1. **Feature engineering** – turning raw data into useful numbers.  \n",
            "2. **Model selection** – picking the right algorithm for the job.  \n",
            "3. **Bias‑variance trade‑off** – balancing under‑ and over‑fitting.  \n",
            "4. **Evaluation metrics** – accuracy, precision, recall, ROC‑AUC, etc.  \n",
            "5. **Deployment** – putting a model into a real application.\n",
            "\n",
            "---\n",
            "\n",
            "### Final Thought\n",
            "\n",
            "Machine Learning isn’t magic—it’s a systematic way of letting data speak for itself. By following the simple loop of “show, guess, check, adjust,” a computer can start to solve complex problems that would be impossible to program by hand. The real fun begins once you start experimenting with different datasets and models!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 3: Cognitive Verifier Pattern\n",
        "# YOU instruct the model to reason carefully and check correctness\n",
        "\n",
        "prompt_v3 = (\n",
        "    \"You are a senior Machine Learning professor.\\n\"\n",
        "    \"Explain Machine Learning for beginners.\\n\"\n",
        "    \"Ensure the explanation is correct, accurate, and has no technical errors.\"\n",
        ")\n",
        "\n",
        "response_v3 = client.chat.completions.create(\n",
        "    model=MODEL,\n",
        "    messages=[{\"role\": \"user\", \"content\": prompt_v3}]\n",
        ")\n",
        "\n",
        "print(\"\\nSTEP 3 OUTPUT:\\n\")\n",
        "print(response_v3.choices[0].message.content)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jDOYYDP1xRaL",
        "outputId": "692b8c89-3013-44e7-bec9-7ccf0c34252e"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "STEP 3 OUTPUT:\n",
            "\n",
            "**Machine Learning (ML) – A Beginner’s Guide**  \n",
            "*Presented by a senior Machine‑Learning professor*\n",
            "\n",
            "---\n",
            "\n",
            "## 1. What is Machine Learning?\n",
            "\n",
            "At its core, **Machine Learning** is a set of techniques that let computers learn patterns from data and use those patterns to make predictions, decisions, or discover hidden structure—without being explicitly programmed for each specific task.\n",
            "\n",
            "> **Key idea** – Instead of writing a rule‑by‑rule program, we feed a computer a large collection of examples (data), let it “learn” a model, and then let the model generalize to new, unseen examples.\n",
            "\n",
            "---\n",
            "\n",
            "## 2. Why Does ML Matter?\n",
            "\n",
            "- **Automation**: Handles repetitive or complex tasks (image recognition, speech transcription, recommendation).\n",
            "- **Scale**: Works on vast amounts of data that would be impossible for humans to analyze manually.\n",
            "- **Adaptivity**: Learns from new data, improving over time.\n",
            "- **Insights**: Reveals hidden patterns (e.g., customer churn, fraud detection).\n",
            "\n",
            "---\n",
            "\n",
            "## 3. The ML Pipeline – A Step‑by‑Step Overview\n",
            "\n",
            "| Step | What Happens | Why It Matters |\n",
            "|------|--------------|----------------|\n",
            "| **1. Define the Problem** | Clarify the goal (classification, regression, clustering, etc.) and success metrics. | Gives direction and sets expectations. |\n",
            "| **2. Gather Data** | Collect raw data from sources (databases, sensors, APIs). | ML only works if there is data. |\n",
            "| **3. Prepare Data** | Clean (remove errors), transform (normalize, encode), and split into training, validation, and test sets. | High‑quality, properly split data prevents bias and leakage. |\n",
            "| **4. Choose a Model** | Pick an algorithm (e.g., linear regression, decision tree, neural network). | Different models fit different problems and data sizes. |\n",
            "| **5. Train the Model** | Adjust the model’s internal parameters so that predictions match known labels. | This is the “learning” phase. |\n",
            "| **6. Evaluate** | Measure performance on unseen validation data using appropriate metrics (accuracy, MSE, F1‑score, etc.). | Checks whether the model generalizes. |\n",
            "| **7. Tune Hyper‑parameters** | Adjust model settings (e.g., depth of a tree, learning rate) to improve performance. | Hyper‑parameters control the learning process itself. |\n",
            "| **8. Deploy** | Integrate the trained model into an application or system. | Makes predictions useful to end‑users. |\n",
            "| **9. Monitor & Update** | Track real‑world performance; retrain with new data when necessary. | Keeps the model relevant as the world changes. |\n",
            "\n",
            "---\n",
            "\n",
            "## 4. Types of Machine Learning\n",
            "\n",
            "| Category | What It Does | Typical Algorithms | Use‑Case Examples |\n",
            "|----------|--------------|--------------------|-------------------|\n",
            "| **Supervised Learning** | Learns a mapping from inputs **x** to outputs **y** using labeled data. | Linear regression, logistic regression, support vector machines, random forests, deep neural nets. | Spam detection, house price prediction, image classification. |\n",
            "| **Unsupervised Learning** | Discovers hidden structure in unlabeled data. | K‑means clustering, hierarchical clustering, PCA, t‑SNE, autoencoders. | Market segmentation, anomaly detection, dimensionality reduction. |\n",
            "| **Semi‑Supervised / Self‑Supervised** | Uses a mix of labeled and unlabeled data, or creates labels from the data itself. | Graph‑based methods, contrastive learning. | Speech recognition with limited labeled data. |\n",
            "| **Reinforcement Learning** | Learns to act in an environment to maximize cumulative reward. | Q‑learning, policy gradients, deep RL. | Game playing (AlphaGo), robotics control. |\n",
            "| **Transfer Learning** | Reuses a model trained on one task to help solve another, related task. | Fine‑tuning pre‑trained CNNs (e.g., ResNet, BERT). | Applying a general image recognizer to a niche medical image task. |\n",
            "\n",
            "---\n",
            "\n",
            "## 5. Core Concepts and Terminology\n",
            "\n",
            "| Term | Definition |\n",
            "|------|------------|\n",
            "| **Feature** | A measurable property or characteristic of the data (e.g., age, pixel intensity). |\n",
            "| **Label/Target** | The quantity we want to predict (e.g., “spam”/“not spam”, price). |\n",
            "| **Training Data** | Portion of the data used to learn the model’s parameters. |\n",
            "| **Validation Data** | Separate set used to tune hyper‑parameters and prevent overfitting. |\n",
            "| **Test Data** | Final set to estimate real‑world performance. |\n",
            "| **Loss Function** | Quantifies the error between predictions and true values; the model’s objective to minimize. |\n",
            "| **Gradient Descent** | Optimization algorithm that iteratively updates parameters in the direction that reduces loss. |\n",
            "| **Overfitting** | When a model captures noise instead of underlying pattern; performs well on training data but poorly on new data. |\n",
            "| **Underfitting** | When a model is too simple to capture the underlying pattern, resulting in poor performance on both training and test data. |\n",
            "| **Regularization** | Techniques (e.g., L1/L2 penalties, dropout) that prevent overfitting by constraining model complexity. |\n",
            "| **Bias‑Variance Trade‑off** | Balancing model’s error due to simplistic assumptions (bias) versus sensitivity to training data (variance). |\n",
            "\n",
            "---\n",
            "\n",
            "## 6. A Simple Illustrative Example\n",
            "\n",
            "**Problem**: Predict whether a customer will churn (leave the service) based on usage data.\n",
            "\n",
            "1. **Collect Data**: For each customer, record features like `monthly_minutes`, `num_calls`, `contract_type`, etc., and the label `churned` (1/0).\n",
            "2. **Preprocess**:\n",
            "   - Encode categorical variables (`contract_type`) into numbers or one‑hot vectors.\n",
            "   - Normalize continuous features.\n",
            "   - Split into 70% training, 15% validation, 15% test.\n",
            "3. **Choose a Model**: Start with a **logistic regression** (good baseline for binary classification).\n",
            "4. **Define Loss**: Use cross‑entropy loss; minimize using gradient descent.\n",
            "5. **Train**: Adjust weights until validation loss stops improving.\n",
            "6. **Evaluate**: Compute accuracy, precision/recall, and ROC‑AUC on the test set.\n",
            "7. **Deploy**: Expose the model as a web API that returns churn probability for a new customer.\n",
            "\n",
            "---\n",
            "\n",
            "## 7. Common Pitfalls to Avoid\n",
            "\n",
            "1. **Data Leakage** – Accidentally feeding the model information from the future or test set (e.g., including a “predicted label” feature).  \n",
            "   *Solution:* Strictly keep training, validation, and test sets separate.\n",
            "\n",
            "2. **Imbalanced Classes** – When one outcome is far more frequent (e.g., 95% non‑churn vs 5% churn).  \n",
            "   *Solution:* Use resampling techniques, appropriate metrics (AUC‑PR), or cost‑sensitive learning.\n",
            "\n",
            "3. **Ignoring Feature Scaling** – Algorithms like k‑NN or SVM are sensitive to scale.  \n",
            "   *Solution:* Standardize or normalize features.\n",
            "\n",
            "4. **Over‑engineering Features** – Creating too many derived features can cause noise and overfitting.  \n",
            "   *Solution:* Use regularization or feature selection.\n",
            "\n",
            "5. **Misinterpreting Correlation as Causation** – ML models capture correlations, not necessarily causal relationships.  \n",
            "   *Solution:* Combine ML with domain knowledge or causal inference methods.\n",
            "\n",
            "---\n",
            "\n",
            "## 8. A Glimpse of Advanced Topics (for those ready to explore further)\n",
            "\n",
            "- **Deep Learning**: Neural networks with many hidden layers (CNNs for images, RNNs/LSTMs for sequences, transformers for language).  \n",
            "- **Explainability (XAI)**: Techniques like SHAP, LIME, and saliency maps that help interpret model decisions.  \n",
            "- **AutoML**: Automated pipeline construction and hyper‑parameter tuning (e.g., Google AutoML, AutoGluon).  \n",
            "- **Federated Learning**: Training models on decentralized data while preserving privacy.  \n",
            "- **Generative Models**: GANs, VAEs that can synthesize new data (e.g., images, music).  \n",
            "\n",
            "---\n",
            "\n",
            "## 9. Quick Take‑away\n",
            "\n",
            "| Key Point | Takeaway |\n",
            "|-----------|----------|\n",
            "| **ML is data‑driven** | Models learn from examples; quality of data is paramount. |\n",
            "| **It’s a cycle** | Define → Collect → Process → Model → Evaluate → Deploy → Monitor. |\n",
            "| **Choose the right tool** | Simpler models (linear, tree) often perform well and are easier to explain. |\n",
            "| **Beware overfitting** | Use validation, regularization, and keep an eye on test performance. |\n",
            "| **Domain knowledge matters** | Feature engineering, data cleaning, and interpretation rely on understanding the problem space. |\n",
            "\n",
            "---\n",
            "\n",
            "### Final Thought\n",
            "\n",
            "Machine Learning is not a magic box—it is a disciplined, iterative process that blends statistical reasoning, algorithmic design, and careful data handling. With patience and practice, you’ll learn to turn raw data into actionable intelligence that can transform products, services, and even entire industries. Happy learning!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 4: Question Refinement Pattern\n",
        "# YOU manually refine a vague question into a specific one\n",
        "\n",
        "prompt_v4 = (\n",
        "    \"You are a senior Machine Learning professor.\\n\"\n",
        "    \"Explain Machine Learning with real-world examples\\n\"\n",
        "    \"specifically for engineering students.\"\n",
        ")\n",
        "\n",
        "response_v4 = client.chat.completions.create(\n",
        "    model=MODEL,\n",
        "    messages=[{\"role\": \"user\", \"content\": prompt_v4}]\n",
        ")\n",
        "\n",
        "print(\"\\nSTEP 4 OUTPUT:\\n\")\n",
        "print(response_v4.choices[0].message.content)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T0PKMP6xxWbQ",
        "outputId": "d0ca1827-2ce2-4755-e6d8-3c99c42d3179"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "STEP 4 OUTPUT:\n",
            "\n",
            "**Machine Learning for Engineering Students – A “Hands‑On” Overview**\n",
            "\n",
            "---\n",
            "\n",
            "### 1.  What is Machine Learning?\n",
            "\n",
            "> **Machine Learning (ML)** is a set of algorithms that *learn* patterns from data and use those patterns to make predictions or decisions, without being explicitly programmed for each specific task.\n",
            "\n",
            "Contrast this with traditional **rule‑based programming**:\n",
            "\n",
            "|  Traditional  |  Machine Learning |\n",
            "|---------------|-------------------|\n",
            "|  Engineers write a set of if‑then rules that cover *all* known cases. |  Engineers supply data and an algorithm that *infers* the rules (the model) automatically. |\n",
            "\n",
            "Because most engineering systems produce massive amounts of data (sensors, logs, images, etc.), ML lets us extract value from that data that would be infeasible to encode manually.\n",
            "\n",
            "---\n",
            "\n",
            "### 2.  Core ML Paradigms & Their Engineering Relevance\n",
            "\n",
            "| Paradigm | Typical Use‑Case in Engineering | Example |\n",
            "|----------|--------------------------------|---------|\n",
            "| **Supervised Learning** | Predict a numerical or categorical output when we have labeled data. | 1. **Fault classification** in a turbine using vibration signatures.<br>2. **Temperature‑to‑power‑curve** regression for a battery pack. |\n",
            "| **Unsupervised Learning** | Discover hidden structure in data without labels. | 1. **Anomaly detection** in network traffic.<br>2. **Clustering** of manufacturing parts for quality control. |\n",
            "| **Semi‑Supervised / Self‑Supervised** | Use a mix of labeled & unlabeled data – useful when labeling is expensive. | 1. **Speech‑to‑text** for automotive voice assistants. |\n",
            "| **Reinforcement Learning (RL)** | Learn a control policy that maximizes a reward signal through interaction. | 1. **Adaptive cruise control** that learns to maintain safe gaps. |\n",
            "| **Transfer Learning** | Re‑use a pre‑trained model on a similar domain to save data & compute. | 1. Use a CNN trained on ImageNet for defect detection in PCB inspection. |\n",
            "\n",
            "---\n",
            "\n",
            "### 3.  A Walk‑Through Example: Predictive Maintenance in a Factory\n",
            "\n",
            "#### 3.1 Problem Statement\n",
            "A manufacturing plant wants to **predict when an industrial motor will fail** so that maintenance can be scheduled before a catastrophic breakdown.\n",
            "\n",
            "#### 3.2 Data Collection\n",
            "- **Sensors:** vibration (Hz), temperature (°C), current (A), speed (rpm).\n",
            "- **Labels:** “failed” or “healthy” for each time window (e.g., a 5‑minute interval).\n",
            "\n",
            "#### 3.3 Feature Engineering\n",
            "- Compute spectral power in specific frequency bands (FFT of vibration).\n",
            "- Rolling mean & standard deviation of temperature.\n",
            "- Correlation between current and speed.\n",
            "\n",
            "*(In many modern pipelines, raw time‑series data can be fed directly into deep nets, but domain knowledge still helps to reduce dimensionality.)*\n",
            "\n",
            "#### 3.4 Model Selection\n",
            "- **Supervised Classification**: Random Forest, Gradient Boosted Trees, or LSTM‑based sequence model.\n",
            "- **Why Random Forest?** Handles noisy, heterogeneous features well, provides feature importance (helpful for explainability).\n",
            "\n",
            "#### 3.5 Training & Validation\n",
            "- Split data into training, validation, test sets using *stratified* sampling (to preserve class imbalance).\n",
            "- Evaluate with **Precision, Recall, F1‑score** and ROC‑AUC (since false negatives are more costly).\n",
            "\n",
            "#### 3.6 Deployment\n",
            "- Embed the trained model on a *edge* device (e.g., PLC with Python support) or run it in a central cloud server.\n",
            "- Use a lightweight inference library (ONNX Runtime, TensorRT).\n",
            "\n",
            "#### 3.7 Outcome\n",
            "- 30% reduction in unplanned downtime.\n",
            "- 25% cost savings in spare parts inventory.\n",
            "\n",
            "---\n",
            "\n",
            "### 4.  Real‑World Engineering Use‑Cases\n",
            "\n",
            "| Domain | ML Technique | Practical Problem | Result |\n",
            "|--------|--------------|--------------------|--------|\n",
            "| **Automotive** | Computer Vision | Detecting road signs & lane lines in self‑driving cars. | Improved navigation accuracy by 95 % vs rule‑based image processing. |\n",
            "| **Electrical Power** | Time‑Series Forecasting | Predicting load demand 24 h ahead for grid management. | Reduced spinning reserve by 12 %. |\n",
            "| **Structural Engineering** | Anomaly Detection | Monitoring bridges for early crack detection via acoustic emission. | Identified 3 critical cracks months before manual inspection. |\n",
            "| **Process Control** | RL (Deep Q‑Learning) | Optimizing chemical reactor temperature & feed rate for maximum yield. | Yield increased by 8 % over heuristic control. |\n",
            "| **Materials Science** | Graph Neural Networks | Predicting material properties (strength, thermal conductivity) from crystal structure. | Enabled discovery of a new alloy with 15 % higher strength. |\n",
            "| **Aerospace** | Transfer Learning | Classifying defects in composite wing skins using drone imagery. | 98 % detection accuracy with only 500 labeled images. |\n",
            "\n",
            "---\n",
            "\n",
            "### 5.  Building an ML Project – A Mini‑Checklist\n",
            "\n",
            "1. **Define the problem & metric.**  \n",
            "   *“Reduce downtime by 20 %” → use *Precision‑Recall* if failures are rare.*\n",
            "\n",
            "2. **Gather & clean data.**  \n",
            "   *Impute missing values, remove outliers, align timestamps.*\n",
            "\n",
            "3. **Explore & visualize.**  \n",
            "   *Use histograms, scatter plots, PCA to understand relationships.*\n",
            "\n",
            "4. **Feature selection / engineering.**  \n",
            "   *Domain‑specific transforms, or use deep nets to learn representations.*\n",
            "\n",
            "5. **Model choice & hyper‑parameter tuning.**  \n",
            "   *Try a baseline (e.g., linear regression), then advanced (XGBoost, CNN).*\n",
            "\n",
            "6. **Cross‑validation & bias‑variance analysis.**  \n",
            "   *Ensure the model generalizes to unseen data.*\n",
            "\n",
            "7. **Deployment considerations.**  \n",
            "   - *Latency* (real‑time vs batch).  \n",
            "   - *Hardware constraints* (edge vs cloud).  \n",
            "   - *Explainability* (important for safety‑critical systems).  \n",
            "\n",
            "8. **Monitoring & maintenance.**  \n",
            "   *Model drift detection, retraining schedule.*\n",
            "\n",
            "---\n",
            "\n",
            "### 6.  Practical Tips for Engineering Students\n",
            "\n",
            "| Tip | Why It Matters |\n",
            "|-----|----------------|\n",
            "| **Start with a simple baseline** | A good “naïve” model (mean predictor, linear regression) gives a benchmark to measure ML gains. |\n",
            "| **Use domain knowledge early** | Feature engineering rooted in physics (e.g., energy balance equations) often outperforms black‑box nets on small data sets. |\n",
            "| **Leverage open‑source libraries** | `scikit-learn`, `pandas`, `torch`, `tensorflow`, `pytorch-lightning`. |\n",
            "| **Document the data pipeline** | Version‑control your raw data (or its metadata) – reproducibility is key. |\n",
            "| **Prioritize explainability** | In safety‑critical fields, you’ll need to show *why* a model made a decision (e.g., SHAP values, LIME). |\n",
            "| **Understand the ethics & safety** | Bias, privacy, and unintended consequences must be evaluated even in “pure” engineering contexts. |\n",
            "\n",
            "---\n",
            "\n",
            "### 7.  Bottom Line\n",
            "\n",
            "- **Machine Learning** is not a magic box; it’s a *tool* that transforms **raw sensor data** into actionable insights.\n",
            "- For engineers, it means **automating complex decision‑making** (e.g., when to replace a component, how to control a robot arm, predicting system reliability).\n",
            "- By integrating ML into the design loop—data → model → deployment—you can unlock **efficiency gains, cost savings, and new capabilities** that were previously impossible with rule‑based systems alone.\n",
            "\n",
            "---\n",
            "\n",
            "**Next Steps for You:**\n",
            "\n",
            "1. Pick a small dataset from your lab or a public repository (e.g., UCI Machine Learning Repository, Kaggle).  \n",
            "2. Define a clear engineering objective, and try a simple model with `scikit-learn`.  \n",
            "3. Iterate, refine, and document every step—this will be the foundation of your engineering‑ML toolkit.\n",
            "\n",
            "Happy modeling!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 5: Provide New Information and Ask Questions\n",
        "# YOU add background context before asking\n",
        "\n",
        "prompt_v5 = (\n",
        "    \"You are a senior Machine Learning professor.\\n\"\n",
        "    \"The learner is a 3rd year engineering student who knows Python basics.\\n\"\n",
        "    \"Explain what Machine Learning is and suggest what they should learn next.\"\n",
        ")\n",
        "\n",
        "response_v5 = client.chat.completions.create(\n",
        "    model=MODEL,\n",
        "    messages=[{\"role\": \"user\", \"content\": prompt_v5}]\n",
        ")\n",
        "\n",
        "print(\"\\nSTEP 5 OUTPUT:\\n\")\n",
        "print(response_v5.choices[0].message.content)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w-fnHrE5xZ_4",
        "outputId": "5c49d36d-b154-43e1-ac7c-68d039b0eb2b"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "STEP 5 OUTPUT:\n",
            "\n",
            "## 1.  What is Machine Learning?  \n",
            "In the simplest terms, **machine learning (ML)** is a set of techniques that let a computer learn patterns from data and then use those patterns to make predictions or decisions without being explicitly programmed for every possible case.\n",
            "\n",
            "| **Key idea** | **What it means in practice** | **Typical example** |\n",
            "|--------------|------------------------------|---------------------|\n",
            "| *Learning from data* | A model is trained on a set of labeled examples (e.g., pictures of cats/dogs). | Image classification |\n",
            "| *Generalization* | After training, the model can correctly label a brand‑new picture it has never seen before. | Spam filter |\n",
            "| *Optimization* | The training process repeatedly adjusts the model’s parameters to reduce a loss (error) function. | Training a linear regression to minimize mean‑squared error |\n",
            "| *Algorithmic families* | **Supervised** (labels available), **unsupervised** (no labels), **reinforcement** (feedback signals). | Regression, clustering, Q‑learning |\n",
            "\n",
            "At a high‑level, the ML pipeline looks like this:\n",
            "\n",
            "1. **Data Collection & Cleaning** – Gather raw observations, handle missing values, remove duplicates.  \n",
            "2. **Feature Engineering** – Extract meaningful variables from raw data (e.g., encode categorical variables, scale numerical ones).  \n",
            "3. **Model Selection** – Choose an algorithm that matches the problem type.  \n",
            "4. **Training** – Fit the model to the training data.  \n",
            "5. **Evaluation** – Measure performance on a held‑out test set (accuracy, RMSE, etc.).  \n",
            "6. **Deployment** – Integrate the model into an application or service.\n",
            "\n",
            "---\n",
            "\n",
            "## 2.  Core Concepts You Should Master\n",
            "\n",
            "| Topic | Why it matters | Suggested starting points |\n",
            "|-------|-----------------|---------------------------|\n",
            "| **Linear algebra & calculus** | ML models are built from mathematical objects (matrices, vectors). | 1‑D numpy for vectors, 2‑D arrays for matrices. Understand dot product, transposes, and basic derivatives. |\n",
            "| **Statistics & probability** | Gives you intuition about data distributions, variance, bias, confidence intervals. | Practice computing mean/variance, understanding Gaussian distribution. |\n",
            "| **Optimization** | Training is all about minimizing a cost function. | Learn gradient descent, stochastic gradient descent, learning rates, convergence. |\n",
            "| **Basic ML algorithms** | They are the building blocks before you dive into neural nets. | Linear regression, logistic regression, k‑Nearest Neighbors, Decision Trees, Random Forests, SVM. |\n",
            "| **Model evaluation & validation** | Avoid overfitting. | Train/validation/test splits, k‑fold cross‑validation, ROC curves, confusion matrix. |\n",
            "| **Feature engineering & preprocessing** | Often the most time‑consuming part of a project. | Scaling (StandardScaler, MinMaxScaler), encoding (OneHotEncoder), handling missing values. |\n",
            "| **Regularization & bias‑variance trade‑off** | Control model complexity. | L1/L2 regularization, dropout for neural nets. |\n",
            "| **Software & reproducibility** | Being able to train and share models is essential. | Git, virtual environments, Jupyter notebooks, reproducible pipelines (MLflow, Sacred). |\n",
            "\n",
            "---\n",
            "\n",
            "## 3.  Suggested Learning Path (3‑6 Months)\n",
            "\n",
            "| Week | Focus | Key Resources | Practical Exercise |\n",
            "|------|-------|---------------|---------------------|\n",
            "| **1‑2** | *Python refresher & NumPy* | *“Python for Data Analysis”* by Wes McKinney | Write your first NumPy matrix operations (dot product, transpose). |\n",
            "| **3‑4** | *Linear Algebra* | Khan Academy “Linear Algebra”, 2‑D arrays | Implement matrix multiplication manually and with NumPy. |\n",
            "| **5‑6** | *Statistics Basics* | *“Think Stats”* by Allen B. Downey | Compute mean, variance, histogram of a sample dataset. |\n",
            "| **7‑9** | *Linear Regression* | Scikit‑learn docs, Andrew Ng ML course (Week 1‑4) | Predict house prices using scikit-learn’s `LinearRegression`. |\n",
            "| **10‑12** | *Logistic Regression & Classification* | Scikit‑learn docs, “Hands‑On Machine Learning” (Ch. 4) | Build a spam classifier from email text data. |\n",
            "| **13‑15** | *Model Evaluation* | Kaggle tutorials, scikit‑learn docs | Implement k‑fold cross‑validation and confusion matrix on the spam dataset. |\n",
            "| **16‑18** | *Decision Trees & Ensemble Methods* | scikit‑learn docs, “Hands‑On Machine Learning” | Train a Random Forest for a multi‑class image dataset. |\n",
            "| **19‑21** | *Feature Engineering* | Kaggle “Feature Engineering” notebooks | Apply one‑hot encoding and scaling to a dataset with mixed types. |\n",
            "| **22‑24** | *Intro to Neural Nets* | DeepLearning.AI’s “Neural Networks and Deep Learning” (Coursera) | Train a simple feed‑forward network on MNIST using PyTorch/TensorFlow. |\n",
            "| **25‑26** | *Project & Deployment* | MLflow tutorial | Create a small end‑to‑end project: data ingestion → preprocessing → model training → evaluation → simple Flask API. |\n",
            "\n",
            "*Why this order?*  \n",
            "You’ll build intuition on how the math translates to code. You’ll also see the “no‑free‑lunch” theorem in practice: a simple linear model often performs surprisingly well if the features are well chosen.\n",
            "\n",
            "---\n",
            "\n",
            "## 4.  Hands‑On Projects to Cement Learning\n",
            "\n",
            "| Project | Skills Covered | Suggested Dataset |\n",
            "|---------|----------------|-------------------|\n",
            "| **Titanic Survival Prediction** | Logistic regression, cross‑validation, imputation | Kaggle: Titanic |\n",
            "| **Customer Churn Prediction** | Feature engineering, Random Forest | Kaggle: Telco Churn |\n",
            "| **Stock Price Forecasting (time series)** | ARIMA basics, lag features | Yahoo Finance API |\n",
            "| **Image Classification (cats vs dogs)** | CNN basics, transfer learning | Kaggle: Dogs vs Cats |\n",
            "| **Recommender System** | Matrix factorization, surprise library | MovieLens 100k |\n",
            "\n",
            "When you finish each project, **write a Jupyter notebook** that includes:\n",
            "\n",
            "1. Problem statement\n",
            "2. Data exploration (EDA)\n",
            "3. Preprocessing steps\n",
            "4. Model training & hyperparameter tuning\n",
            "5. Evaluation metrics\n",
            "6. Discussion of limitations & next steps\n",
            "\n",
            "---\n",
            "\n",
            "## 5.  Recommended Libraries & Tools\n",
            "\n",
            "| Purpose | Library | Why it’s useful |\n",
            "|---------|---------|-----------------|\n",
            "| Data manipulation | pandas | DataFrames, grouping, joins |\n",
            "| Numerical ops | NumPy | Fast array math |\n",
            "| Plotting | Matplotlib / Seaborn | Visualize data and results |\n",
            "| ML models | scikit‑learn | Quick prototyping |\n",
            "| Deep learning | PyTorch / TensorFlow | GPU accelerated training |\n",
            "| Hyperparameter search | Optuna / Hyperopt | Efficient tuning |\n",
            "| Pipelines & reproducibility | MLflow / DVC | Track experiments |\n",
            "| NLP (if you go there) | spaCy / HuggingFace | Tokenization, embeddings |\n",
            "\n",
            "---\n",
            "\n",
            "## 6.  Advanced Topics (Once You’re Comfortable)\n",
            "\n",
            "1. **Unsupervised Learning** – PCA, t‑SNE, clustering (K‑Means, DBSCAN).  \n",
            "2. **Deep Learning** – CNNs, RNNs, transformers.  \n",
            "3. **Model Interpretability** – SHAP, LIME.  \n",
            "4. **Reinforcement Learning** – Q‑learning, policy gradients.  \n",
            "5. **Scalable ML** – Spark MLlib, TensorFlow‑On‑Kubernetes.  \n",
            "6. **Ethics & Bias** – Fairness, accountability, transparency.  \n",
            "\n",
            "You can explore these through courses on Coursera, Udacity, or the fast.ai “Practical Deep Learning for Coders” series.\n",
            "\n",
            "---\n",
            "\n",
            "## 7.  Quick Checklist for Your Learning Journey\n",
            "\n",
            "- [ ] Understand how a linear model works mathematically and in code.  \n",
            "- [ ] Build a full ML pipeline in scikit‑learn from preprocessing to evaluation.  \n",
            "- [ ] Implement gradient descent manually (for a simple regression).  \n",
            "- [ ] Train a neural net on a basic image dataset.  \n",
            "- [ ] Use cross‑validation and grid search for hyperparameter tuning.  \n",
            "- [ ] Document a project in a reproducible notebook and share it (GitHub, Kaggle).  \n",
            "\n",
            "Once you tick those boxes, you’ll have a solid foundation to tackle more complex problems and to start contributing to research or industry projects.\n",
            "\n",
            "---\n",
            "\n",
            "### Final Thought\n",
            "\n",
            "Machine Learning is *not* a single algorithm; it’s a **toolkit** that blends mathematics, software engineering, and domain knowledge. By building small, end‑to‑end projects, you’ll gain intuition that will guide you whether you pursue deeper theoretical work or practical system building.  \n",
            "\n",
            "Happy coding and happy learning! 🚀\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 6: Root Prompt\n",
        "# YOU enforce global rules on the response\n",
        "\n",
        "prompt_v6 = (\n",
        "    \"You are an exam assistant.\\n\"\n",
        "    \"Rules:\\n\"\n",
        "    \"- Use bullet points\\n\"\n",
        "    \"- Use simple language\\n\"\n",
        "    \"- Maximum 5 points\\n\\n\"\n",
        "    \"Explain overfitting in Machine Learning.\"\n",
        ")\n",
        "\n",
        "response_v6 = client.chat.completions.create(\n",
        "    model=MODEL,\n",
        "    messages=[{\"role\": \"user\", \"content\": prompt_v6}]\n",
        ")\n",
        "\n",
        "print(\"\\nSTEP 6 OUTPUT:\\n\")\n",
        "print(response_v6.choices[0].message.content)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ylPzkX-xgO8",
        "outputId": "0535e542-35a1-4241-8c65-a46632280ce2"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "STEP 6 OUTPUT:\n",
            "\n",
            "- **What it is**: Overfitting happens when a model learns the training data too well, even the random noise, and can’t handle new data.  \n",
            "- **Why it happens**: Too many parameters, too complex a model, or too little training data let the model capture details that aren’t true patterns.  \n",
            "- **Result**: The model scores high on training data but does poorly on unseen (test) data.  \n",
            "- **Common signs**: Training accuracy is near 100 % but test accuracy is much lower.  \n",
            "- **Ways to avoid it**: Use simpler models, add more training data, employ regularization, or split data into training/validation/test sets.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "EXERCISE 4"
      ],
      "metadata": {
        "id": "nPGy_0kfxsNu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 1: Zero-Shot Prompting\n",
        "# No examples given\n",
        "\n",
        "prompt_1 = \"Classify the following word: Apple\"\n",
        "\n",
        "response_1 = client.chat.completions.create(\n",
        "    model=MODEL,\n",
        "    messages=[{\"role\": \"user\", \"content\": prompt_1}]\n",
        ")\n",
        "\n",
        "print(\"STEP 1 OUTPUT:\\n\")\n",
        "print(response_1.choices[0].message.content)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Os_pB21xuZi",
        "outputId": "0e8deb78-f31a-4953-a93e-b66874f03e4e"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "STEP 1 OUTPUT:\n",
            "\n",
            "**Apple**\n",
            "\n",
            "- **Part of Speech**: Noun  \n",
            "- **Common Usage**: A fruit (e.g., “an apple” in English).  \n",
            "- **Proper Noun**: The name of the technology company (e.g., “Apple Inc.”).  \n",
            "- **Semantic Category**: Fruit → Edible plant product.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 2: One-Shot Prompting\n",
        "# YOU provide one example\n",
        "\n",
        "prompt_2 = (\n",
        "    \"Cat -> Animal\\n\"\n",
        "    \"Apple ->\"\n",
        ")\n",
        "\n",
        "response_2 = client.chat.completions.create(\n",
        "    model=MODEL,\n",
        "    messages=[{\"role\": \"user\", \"content\": prompt_2}]\n",
        ")\n",
        "\n",
        "print(\"\\nSTEP 2 OUTPUT:\\n\")\n",
        "print(response_2.choices[0].message.content)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-pV3H-znxwjb",
        "outputId": "0dd46261-9007-4591-8e4c-c0e26178c601"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "STEP 2 OUTPUT:\n",
            "\n",
            "Apple -> Fruit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 3: Few-Shot Prompting\n",
        "# YOU provide multiple examples\n",
        "\n",
        "prompt_3 = (\n",
        "    \"Cat -> Animal\\n\"\n",
        "    \"Rose -> Flower\\n\"\n",
        "    \"Apple ->\"\n",
        ")\n",
        "\n",
        "response_3 = client.chat.completions.create(\n",
        "    model=MODEL,\n",
        "    messages=[{\"role\": \"user\", \"content\": prompt_3}]\n",
        ")\n",
        "\n",
        "print(\"\\nSTEP 3 OUTPUT:\\n\")\n",
        "print(response_3.choices[0].message.content)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EKsFVnugx1xh",
        "outputId": "9a657312-409c-4e41-859e-b249991e9df3"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "STEP 3 OUTPUT:\n",
            "\n",
            "Apple -> Fruit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 3: Few-Shot Prompting\n",
        "# YOU provide multiple examples\n",
        "\n",
        "prompt_3 = (\n",
        "    \"Cat -> Animal\\n\"\n",
        "    \"Rose -> Flower\\n\"\n",
        "    \"peacock ->\"\n",
        ")\n",
        "\n",
        "response_3 = client.chat.completions.create(\n",
        "    model=MODEL,\n",
        "    messages=[{\"role\": \"user\", \"content\": prompt_3}]\n",
        ")\n",
        "\n",
        "print(\"\\nSTEP 3 OUTPUT:\\n\")\n",
        "print(response_3.choices[0].message.content)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t4YM0Ijix418",
        "outputId": "eee8fc73-366a-4eba-eb93-21310c096544"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "STEP 3 OUTPUT:\n",
            "\n",
            "Peacock → Bird\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 4: Chain of Thought\n",
        "# YOU explicitly request step-by-step reasoning\n",
        "\n",
        "prompt_4 = (\n",
        "    \"A model has 80% accuracy on 200 samples.\\n\"\n",
        "    \"Calculate how many predictions are correct.\\n\"\n",
        "    \"Show the reasoning step by step.\"\n",
        ")\n",
        "\n",
        "response_4 = client.chat.completions.create(\n",
        "    model=MODEL,\n",
        "    messages=[{\"role\": \"user\", \"content\": prompt_4}]\n",
        ")\n",
        "\n",
        "print(\"\\nSTEP 4 OUTPUT:\\n\")\n",
        "print(response_4.choices[0].message.content)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1QHgX3H0ybuw",
        "outputId": "eedcf45b-9a94-4e43-d8ac-4f556dbb4838"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "STEP 4 OUTPUT:\n",
            "\n",
            "**Step‑by‑step calculation**\n",
            "\n",
            "1. **Understand the definition of accuracy**  \n",
            "   \\[\n",
            "   \\text{Accuracy} = \\frac{\\text{Number of correct predictions}}{\\text{Total number of predictions}} \\times 100\\%\n",
            "   \\]\n",
            "\n",
            "2. **Set up the known values**  \n",
            "   - Accuracy given: \\(80\\%\\)  \n",
            "   - Total predictions (samples): \\(200\\)\n",
            "\n",
            "3. **Convert the percentage to a decimal**  \n",
            "   \\[\n",
            "   80\\% = \\frac{80}{100} = 0.80\n",
            "   \\]\n",
            "\n",
            "4. **Apply the formula to find the number of correct predictions**  \n",
            "   \\[\n",
            "   \\text{Number of correct predictions} = 0.80 \\times 200\n",
            "   \\]\n",
            "\n",
            "5. **Perform the multiplication**  \n",
            "   \\[\n",
            "   0.80 \\times 200 = 160\n",
            "   \\]\n",
            "\n",
            "**Result**\n",
            "\n",
            "The model made **160 correct predictions** out of 200 samples.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 5: Tabular Format\n",
        "# YOU demand a table format\n",
        "\n",
        "prompt_5 = \"Compare supervised and unsupervised learning in a table.\"\n",
        "\n",
        "response_5 = client.chat.completions.create(\n",
        "    model=MODEL,\n",
        "    messages=[{\"role\": \"user\", \"content\": prompt_5}]\n",
        ")\n",
        "\n",
        "print(\"\\nSTEP 5 OUTPUT:\\n\")\n",
        "print(response_5.choices[0].message.content)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l8yUdVmcynA6",
        "outputId": "8126b58a-816b-4273-968b-68a585d1d4da"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "STEP 5 OUTPUT:\n",
            "\n",
            "Below is a concise side‑by‑side comparison of **supervised** and **unsupervised** learning in a single markdown table:\n",
            "\n",
            "| **Aspect** | **Supervised Learning** | **Unsupervised Learning** |\n",
            "|------------|--------------------------|---------------------------|\n",
            "| **Definition** | Learns a mapping from inputs to known target outputs (labels). | Learns intrinsic structure of inputs without any target labels. |\n",
            "| **Input data** | Feature vectors **with** associated labels. | Feature vectors **without** labels. |\n",
            "| **Output** | Predicted label or continuous value for new data (classification/regression). | Groups, embeddings, density estimates, or other representations of the data. |\n",
            "| **Typical tasks** | Classification, regression, ranking, time‑series forecasting. | Clustering, dimensionality reduction, anomaly detection, association mining. |\n",
            "| **Common algorithms** | Logistic regression, SVM, Random Forest, Gradient‑Boosted Trees, Neural nets, Linear/Non‑linear regression. | k‑Means, Hierarchical clustering, DBSCAN, Gaussian Mixture Models, PCA, t‑SNE, Autoencoders. |\n",
            "| **Evaluation metrics** | Accuracy, precision/recall, F1, ROC‑AUC, MAE, RMSE, R². | Silhouette score, elbow method, Davies–Bouldin, reconstruction error, visual inspection. |\n",
            "| **Label requirement** | Requires a large, high‑quality labeled dataset. | No labels needed; only raw features. |\n",
            "| **Training complexity** | Often computationally heavier due to loss‑function optimization and hyper‑parameter tuning; may need more data. | Usually simpler, but can be sensitive to parameter choices (e.g., number of clusters). |\n",
            "| **Typical use‑cases** | Spam detection, image classification, medical diagnosis, credit scoring, weather prediction. | Market segmentation, customer profiling, anomaly detection in networks, exploratory data analysis. |\n",
            "| **Advantages** | Clear, objective performance metrics; direct applicability to prediction problems. | Can discover hidden patterns, no need for costly labeling, useful for exploratory tasks. |\n",
            "| **Limitations** | Requires labeled data, risk of over‑fitting, less flexible if labels are ambiguous. | Hard to validate results objectively; may produce clusters that are not meaningful to the domain. |\n",
            "| **When to choose** | You have a well‑defined target variable and want accurate predictions. | You want to explore the data structure or lack labeled examples. |\n",
            "\n",
            "Feel free to tweak the rows or add more categories that are relevant to your specific context!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 6: Fill in the Blank\n",
        "\n",
        "prompt_6 = \"Machine Learning is a subset of ______.\"\n",
        "\n",
        "response_6 = client.chat.completions.create(\n",
        "    model=MODEL,\n",
        "    messages=[{\"role\": \"user\", \"content\": prompt_6}]\n",
        ")\n",
        "\n",
        "print(\"\\nSTEP 6 OUTPUT:\\n\")\n",
        "print(response_6.choices[0].message.content)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pRxkOSw2yqgD",
        "outputId": "d4503e9e-b76a-4c28-efc1-4be1ab377f8e"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "STEP 6 OUTPUT:\n",
            "\n",
            "Machine Learning is a subset of **Artificial Intelligence**.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 7: RGC Prompting\n",
        "# YOU define role, goal, and constraints\n",
        "\n",
        "prompt_7 = (\n",
        "    \"Role: AI Tutor\\n\"\n",
        "    \"Goal: Explain bias-variance tradeoff\\n\"\n",
        "    \"Constraint: Use exactly 3 bullet points\"\n",
        ")\n",
        "\n",
        "response_7 = client.chat.completions.create(\n",
        "    model=MODEL,\n",
        "    messages=[{\"role\": \"user\", \"content\": prompt_7}]\n",
        ")\n",
        "\n",
        "print(\"\\nSTEP 7 OUTPUT:\\n\")\n",
        "print(response_7.choices[0].message.content)\n"
      ],
      "metadata": {
        "id": "prFnrBuDywmZ",
        "outputId": "12b00985-2721-4bf7-abc6-8580c358a4da",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "STEP 7 OUTPUT:\n",
            "\n",
            "- **Bias**: systematic error that arises when a model makes strong simplifying assumptions; high bias leads to *underfitting*, where the model cannot capture the true underlying patterns (typically low variance).  \n",
            "- **Variance**: sensitivity of a model’s predictions to fluctuations in the training data; high variance produces *overfitting*, where the model captures noise (typically low bias).  \n",
            "- **Trade‑off**: the goal is to choose a model complexity that balances bias and variance so that the total expected prediction error is minimized—this is achieved through techniques such as cross‑validation, regularization, and model selection.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AyTyJ79Fyy9K"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}